# 0x05. Regularization

## What is Regularization?
In general, regularization means to make things regular or acceptable. This is exactly why we use it for applied machine learning. The term ‘regularization’ refers to a set of techniques that regularizes learning from particular features for traditional algorithms or neurons in the case of neural network algorithms.

It normalizes and moderates weights attached to a feature or a neuron so that algorithms do not rely on just a few features or neurons to predict the result. This technique helps to avoid the problem of overfitting.

## Why do we need regularization?
The goal of our machine learning algorithm is to learn the data patterns and ignore the noise in the data set and to solve such cases.

## Different Regularization techniques in Deep Learning:
* L1 and L2 regularization
* Dropout
* Data augmentation
* Early stopping
